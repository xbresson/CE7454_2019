{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03 : Policy Network with Global Reward - demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    file_name = 'policy_global_reward_demo.ipynb'\n",
    "    import subprocess\n",
    "    path_to_file = subprocess.check_output('find . -type f -name ' + str(file_name), shell=True).decode(\"utf-8\")\n",
    "    print(path_to_file)\n",
    "    path_to_file = path_to_file.replace(file_name,\"\").replace('\\n',\"\")\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(torch.randint(10000,())) # random seed for pythorch random generator\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import gym\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "from collections import namedtuple\n",
    "import random\n",
    "from itertools import count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init state: [ 0.03073904  0.00145001 -0.03088818 -0.03131252]\n",
      "t= 0 action= 1 state= [ 0.03077  0.197   -0.03151 -0.33358] reward= 1.0 done= False\n",
      "t= 1 action= 0 state= [ 0.03471  0.00234 -0.03819 -0.051  ] reward= 1.0 done= False\n",
      "t= 2 action= 0 state= [ 0.03475 -0.19221 -0.03921  0.2294 ] reward= 1.0 done= False\n",
      "t= 3 action= 0 state= [ 0.03091 -0.38675 -0.03462  0.50946] reward= 1.0 done= False\n",
      "t= 4 action= 1 state= [ 0.02318 -0.19116 -0.02443  0.20607] reward= 1.0 done= False\n",
      "t= 5 action= 1 state= [ 0.01935  0.0043  -0.02031 -0.09422] reward= 1.0 done= False\n",
      "t= 6 action= 0 state= [ 0.01944 -0.19052 -0.02219  0.19199] reward= 1.0 done= False\n",
      "t= 7 action= 1 state= [ 0.01563  0.00491 -0.01835 -0.10761] reward= 1.0 done= False\n",
      "t= 8 action= 1 state= [ 0.01573  0.20029 -0.0205  -0.40603] reward= 1.0 done= False\n",
      "t= 9 action= 1 state= [ 0.01973  0.3957  -0.02862 -0.7051 ] reward= 1.0 done= False\n",
      "t= 10 action= 1 state= [ 0.02765  0.5912  -0.04273 -1.00666] reward= 1.0 done= False\n",
      "t= 11 action= 1 state= [ 0.03947  0.78687 -0.06286 -1.31244] reward= 1.0 done= False\n",
      "t= 12 action= 1 state= [ 0.05521  0.98273 -0.08911 -1.62412] reward= 1.0 done= False\n",
      "t= 13 action= 0 state= [ 0.07486  0.78876 -0.12159 -1.36049] reward= 1.0 done= False\n",
      "t= 14 action= 1 state= [ 0.09064  0.98518 -0.1488  -1.6886 ] reward= 1.0 done= False\n",
      "t= 15 action= 1 state= [ 0.11034  1.18167 -0.18257 -2.02367] reward= 1.0 done= False\n",
      "t= 16 action= 0 state= [ 0.13397  0.98885 -0.22305 -1.79263] reward= 1.0 done= True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Env parameters\n",
    "env_seed = 1\n",
    "render = True # display on\n",
    "render = False # display off\n",
    "\n",
    "#Initialize the environment with the same seed/initialization value\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(env_seed)\n",
    "\n",
    "#Reset the environment\n",
    "state = env.reset() \n",
    "print('init state:',state)\n",
    "\n",
    "#Rollout one episode until it finishes \n",
    "for t in count():  \n",
    "    action = torch.LongTensor(1).random_(0,2).item() # randomly generated action=a in {0,1}\n",
    "    state, reward, done, _ = env.step(action) # receive next state=s' and reward=r\n",
    "    print('t=',t, 'action=',action, 'state=',np.array_str(state, precision=5), 'reward=',reward, 'done=',done )\n",
    "    if render:\n",
    "        env.render() # see the state\n",
    "    if done:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy_NN(\n",
      "  (fc1): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "Policy_NN(\n",
      "  (fc1): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "batch_episode_lengths: [11, 18, 15]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# class of policy network\n",
    "class Policy_NN(nn.Module): \n",
    "    \n",
    "    def __init__(self, net_parameters):\n",
    "        super(Policy_NN, self).__init__()\n",
    "        input_dim = net_parameters['input_dim']\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        output_dim = net_parameters['output_dim']\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        actions_score = self.fc2(x)\n",
    "        actions_prob = torch.softmax(actions_score, dim=1)\n",
    "        return actions_prob\n",
    "    \n",
    "    def select_action(self, state): # select action w/ policy network\n",
    "        probs = self.forward(state) # probability of action a in state s\n",
    "        bernoulli_sampling = torch.distributions.Categorical(probs) \n",
    "        action = bernoulli_sampling.sample() # sample action a with Bernoulli sampling\n",
    "        log_prob = bernoulli_sampling.log_prob(action) # compute log prob of selected action\n",
    "        action = action.item()\n",
    "        return action, log_prob\n",
    "    \n",
    "    def loss(self, batch_rewards, batch_log_probs, batch_rewards_baseline):\n",
    "        nb_episodes_per_batch = len(batch_rewards)\n",
    "        batch_episode_rewards = torch.Tensor(batch_rewards)\n",
    "        batch_episode_rewards_baseline = torch.Tensor(batch_rewards_baseline)\n",
    "        batch_episode_rewards -= batch_episode_rewards_baseline # compare current model w.r.t. baseline model\n",
    "        batch_policy_losses = []\n",
    "        for episode in range(nb_episodes_per_batch):\n",
    "            episode_reward = batch_episode_rewards[episode]\n",
    "            episode_log_probs = torch.stack(batch_log_probs[episode])\n",
    "            policy_loss = - episode_log_probs.sum() * episode_reward\n",
    "            batch_policy_losses.append(policy_loss)\n",
    "        loss = torch.stack(batch_policy_losses).mean()\n",
    "        return loss\n",
    "        \n",
    "\n",
    "        \n",
    "# class of rollout episodes\n",
    "class Rollout_Episodes():\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Rollout_Episodes, self).__init__()\n",
    "        \n",
    "    def rollout_batch_episodes(self, env, opt_parameters, policy_net):\n",
    "        # storage structure of all episodes (w/ different lengths):\n",
    "        #   batch_rewards =         [    - ,       -      , ...,    -   ]\n",
    "        #   batch_log_probs =       [ [- - -] , [- - - -] , ... , [- -] ]\n",
    "        #   batch_episode_lengths = [    - ,       -      , ...,    -   ]\n",
    "        nb_episodes_per_batch = opt_parameters['nb_episodes_per_batch']\n",
    "        env_seeds = opt_parameters['env_seed']\n",
    "        batch_rewards = []\n",
    "        batch_log_probs = []\n",
    "        batch_episode_lengths = []\n",
    "        for episode in range(nb_episodes_per_batch):\n",
    "            rewards = []\n",
    "            log_probs = []\n",
    "            env.seed(env_seeds[episode].item()) # start with random seed\n",
    "            state = env.reset() # reset environment\n",
    "            for t in range(1000): # rollout one episode\n",
    "                state_pytorch = torch.from_numpy(state).float().unsqueeze(0) # state=s\n",
    "                action, log_prob = policy_net.select_action(state_pytorch) # select action=a from state=s\n",
    "                state, reward, done, _ = env.step(action) # receive next state=s' and reward=r\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(log_prob)\n",
    "                if done:\n",
    "                    batch_episode_lengths.append(t)\n",
    "                    batch_rewards.append(t)\n",
    "                    batch_log_probs.append(log_probs)\n",
    "                    break\n",
    "        return batch_rewards, batch_log_probs, batch_episode_lengths\n",
    "\n",
    "        \n",
    "    \n",
    "# network parameters\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 4\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['output_dim'] = 2\n",
    "\n",
    "# instantiate network\n",
    "policy_net = Policy_NN(net_parameters)\n",
    "print(policy_net)\n",
    "baseline_policy_net = Policy_NN(net_parameters)\n",
    "baseline_policy_net.load_state_dict(policy_net.state_dict())\n",
    "baseline_policy_net = baseline_policy_net.eval()\n",
    "print(baseline_policy_net)\n",
    "\n",
    "\n",
    "# instantiate rollout\n",
    "rollout_policy_net = Rollout_Episodes()\n",
    "rollout_baseline_policy_net = Rollout_Episodes()\n",
    "\n",
    "# optimization parameters\n",
    "opt_parameters = {}\n",
    "opt_parameters['nb_episodes_per_batch'] = 3\n",
    "opt_parameters['env_seed'] = torch.LongTensor(opt_parameters['nb_episodes_per_batch']).random_(1,10000)\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "batch_rewards, batch_log_probs, batch_episode_lengths = rollout_policy_net.rollout_batch_episodes(env, opt_parameters, policy_net)\n",
    "#print('batch_rewards:',batch_rewards)\n",
    "#print('batch_log_probs:',batch_log_probs)\n",
    "print('batch_episode_lengths:',batch_episode_lengths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_episode_lengths: [17, 19, 9]\n",
      "batch_episode_lengths: [12, 28, 16]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# optimization parameters\n",
    "opt_parameters = {}\n",
    "opt_parameters['lr'] = 0.001\n",
    "opt_parameters['nb_episodes_per_batch'] = 3\n",
    "opt_parameters['nb_batches_per_epoch'] = 10\n",
    "opt_parameters['env_seed'] = torch.LongTensor(opt_parameters['nb_episodes_per_batch']).random_(1,10000)\n",
    "opt_parameters['gamma'] = 0.99\n",
    "\n",
    "batch_rewards, batch_log_probs, batch_episode_lengths = rollout_policy_net.rollout_batch_episodes(env, opt_parameters, policy_net)\n",
    "print('batch_episode_lengths:',batch_episode_lengths)\n",
    "\n",
    "batch_rewards_baseline, batch_log_probs_baseline, batch_episode_lengths_baseline = rollout_baseline_policy_net.rollout_batch_episodes(env, opt_parameters, policy_net)\n",
    "print('batch_episode_lengths:',batch_episode_lengths_baseline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(-35.9107, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loss\n",
    "loss = policy_net.loss(batch_rewards, batch_log_probs, batch_rewards_baseline)\n",
    "print('loss:',loss)\n",
    "\n",
    "# Backward pass\n",
    "lr = opt_parameters['lr']\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=lr)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(env, policy_net, baseline_policy_net, opt_parameters):\n",
    "    \"\"\"\n",
    "    train one epoch\n",
    "    \"\"\"    \n",
    "    policy_net.train()\n",
    "    baseline_policy_net.eval()\n",
    "    rollout_policy = Rollout_Episodes()\n",
    "    rollout_baseline_policy = Rollout_Episodes()\n",
    "    epoch_loss = 0\n",
    "    nb_data = 0\n",
    "    epoch_episode_length = 0\n",
    "    epoch_episode_lengths = []\n",
    "    nb_batches_per_epoch = opt_parameters['nb_batches_per_epoch']\n",
    "    for iter in range(nb_batches_per_epoch):\n",
    "        opt_parameters['env_seed'] = torch.LongTensor(opt_parameters['nb_episodes_per_batch']).random_(1,10000)\n",
    "        batch_rewards, batch_log_probs, batch_episode_lengths = \\\n",
    "            rollout_policy.rollout_batch_episodes(env, opt_parameters, policy_net)\n",
    "        batch_rewards_baseline, batch_log_probs_baseline, batch_episode_lengths_baseline = \\\n",
    "            rollout_baseline_policy.rollout_batch_episodes(env, opt_parameters, baseline_policy_net)\n",
    "        loss = policy_net.loss(batch_rewards, batch_log_probs, batch_rewards_baseline)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        nb_data += len(batch_episode_lengths)\n",
    "        epoch_episode_length += torch.tensor(batch_episode_lengths).float().sum()\n",
    "        epoch_episode_lengths.append(epoch_episode_length)\n",
    "    epoch_loss /= nb_data\n",
    "    epoch_episode_length /= nb_data\n",
    "    return epoch_loss, epoch_episode_length, epoch_episode_lengths\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATE BASELINE - epoch: 48\n",
      "Training done.\n",
      "Last episode length is 390.6000061035156, epoch is 48\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# network parameters\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 4\n",
    "net_parameters['hidden_dim'] = 256\n",
    "net_parameters['output_dim'] = 2\n",
    "\n",
    "# instantiate network\n",
    "policy_net = Policy_NN(net_parameters)\n",
    "print(policy_net)\n",
    "baseline_policy_net = Policy_NN(net_parameters)\n",
    "baseline_policy_net = baseline_policy_net.eval()\n",
    "print(baseline_policy_net)\n",
    "\n",
    "# optimization parameters\n",
    "opt_parameters = {}\n",
    "opt_parameters['lr'] = 0.0005\n",
    "opt_parameters['nb_episodes_per_batch'] = 5\n",
    "opt_parameters['nb_batches_per_epoch'] = 10\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=opt_parameters['lr'])\n",
    "\n",
    "# select maximum episode length to learn\n",
    "env = gym.make('CartPole-v0')\n",
    "env._max_episode_steps = 400 # 200 400\n",
    "env.spec.reward_threshold = 0.975* env._max_episode_steps\n",
    "print('env._max_episode_steps',env._max_episode_steps)\n",
    "\n",
    "# train loop\n",
    "all_epoch_lengths = []\n",
    "start = time.time()\n",
    "for epoch in range(500): \n",
    "    \n",
    "    # train one epoch\n",
    "    epoch_train_loss, epoch_episode_length, epoch_episode_lengths = \\\n",
    "        train_one_epoch(env, policy_net, baseline_policy_net, opt_parameters)\n",
    "\n",
    "    # update baseline if current policy better\n",
    "    if epoch>0 and (not epoch%2):\n",
    "        opt_parameters['env_seed'] = torch.LongTensor(10).random_(1,10000)\n",
    "        _, _, batch_episode_lengths = Rollout_Episodes().rollout_batch_episodes(env, opt_parameters, policy_net)\n",
    "        _, _, batch_episode_lengths_baseline = Rollout_Episodes().rollout_batch_episodes(env, opt_parameters, baseline_policy_net)\n",
    "        if torch.Tensor(batch_episode_lengths).mean() > torch.Tensor(batch_episode_lengths_baseline).mean():\n",
    "            print('UPDATE BASELINE - epoch:',epoch)\n",
    "            baseline_policy_net.load_state_dict(policy_net.state_dict())\n",
    "        else:\n",
    "            print('NO UPDATE BASELINE - epoch:',epoch)\n",
    "    \n",
    "    # stop training when reward is high\n",
    "    if epoch_episode_length > env.spec.reward_threshold:\n",
    "        print('Training done.')\n",
    "        print(\"Last episode length is {}, epoch is {}\".\n",
    "              format(epoch_episode_length, epoch))\n",
    "        break\n",
    "\n",
    "    # print intermediate info\n",
    "    if not epoch%1:\n",
    "        print('Epoch: {}, time: {:.4f}, train_loss: {:.4f}, episode_length: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_episode_length))\n",
    "        print('           policy net eval: {:.4f}, baseline policy net eval: {:.4f}'.format(torch.Tensor(batch_episode_lengths).mean().item(), torch.Tensor(batch_episode_lengths_baseline).mean().item() ))\n",
    "      \n",
    "    # plot all epochs\n",
    "    all_epoch_lengths.append(epoch_episode_length)\n",
    "    if not epoch%1:\n",
    "        plt.figure(2)\n",
    "        plt.title('Training...')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Length of episodes batch')\n",
    "        plt.plot(torch.Tensor(all_epoch_lengths).numpy())\n",
    "        plt.pause(0.001)\n",
    "        display.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last episode length is 390.6000061035156, epoch is 48\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deXiU9bn/8fedjbAEAhJ2wo6AskfBfW1Fq1VbtVqtaK1o66laba2en11s9dTaVque01qsC9YVbRW1ilIUd5Eg+75DIISwJIQl+/37Y57EAFmGkMkkmc/ruuaaeb7PMveMknue72rujoiICEBctAMQEZGmQ0lBREQqKSmIiEglJQUREamkpCAiIpWUFEREpJKSgshhMLN4M9tjZukNeaxIU2EapyAtmZntqbLZBigCyoLtG9z9ucaPSqTpUlKQmGFm64EfuPt/ajkmwd1LGy8qkaZF1UcS08zsXjN7ycxeMLMC4CozO8HMPjezPDPLNrNHzCwxOD7BzNzM+gbbzwb73zazAjP7zMz6He6xwf5zzWylmeWb2aNm9omZXdO434jEOiUFEbgYeB7oALwElAK3AJ2Bk4AJwA21nP9d4BdAJ2Aj8NvDPdbMugBTgZ8F77sOOL6+H0ikvpQUROBjd3/D3cvdfb+7z3H32e5e6u5rgcnAabWc/4q7Z7p7CfAcMKoex54PzHf3acG+h4DtR/7RRA5PQrQDEGkCNlXdMLMhwJ+AsYQapxOA2bWcv7XK631Au3oc26NqHO7uZpZVZ+QiDUx3CiJwcG+LvwGLgYHu3h74JWARjiEb6FWxYWYG9Izwe4ocQklB5FApQD6w18yGUnt7QkN5ExhjZheYWQKhNo20RnhfkQMoKYgc6nZgIlBA6K7hpUi/obvnAN8BHgR2AAOAeYTGVWBmp5tZXsXxZvYLM3ujyva7ZnZHpOOUlk/jFESaIDOLB7YAl7j7R9GOR2KH7hREmggzm2BmHcysFaFuq6XAF1EOS2JMxJNCMP/LPDN7M9juZ2azzWxVMGgoKShvFWyvDvb3jXRsIk3MycBaQl1RJwAXuXtRdEOSWNMYdwq3AMuqbP8eeMjdBwG7gOuC8uuAXe4+kFAf7d83QmwiTYa73+3uR7l7iruPd/c50Y5JYk9Ek4KZ9QK+Afw92DbgTOCV4JApwEXB6wuDbYL9ZwXHi4hII4n04LU/A3cQ6uIHcBSQV2XCsSy+6ovdk2DwjruXmll+cHyNozo7d+7sffv2jUDYIiIt19y5c7e7e7VdniOWFMzsfGCbu881s9Mriqs51MPYV/W6k4BJAOnp6WRmZjZAtCIiscPMNtS0L5LVRycB3wymK36RULXRn4HUYHAOhEZwbgleZwG9ITS7JKHJyXYefFF3n+zuGe6ekZamsT0iIg0pYknB3e9y917u3he4HHjP3a8E3gcuCQ6bCEwLXr8ebBPsf881iEJEpFFFY5zCz4HbzGw1oTaDJ4LyJ4CjgvLbgDujEJuISExrlFlS3X0WMCt4vZZq5ol390Lg0saIR0REqqcRzSIiUklJQUREKikpiIhIJSUFEZFm5uH/rGJhVl7dB9aDluMUEWlGPlqVy0P/WUlZeTkjeqU2+PV1pyAi0kwUlpRx92uL6de5LT86Y2BE3kN3CiIizcSj761iw459PP+DcSQnxkfkPXSnICLSDKzMKeBvH6zlW2N6cuLAzhF7H90piIhESHm5s3BzPjOWbmVLXiG/+9bwev3CLy937vrXIlKSE7j7G8MiEOlXlBRERBpQcWk5n63dwYylW5mxNIec3UWYgTucPLAz3x7b67Cv+eKcTczdsIs/XDKCTm2TIhD1V5QUREQayIMzVvLUx+soKCqlTVI8pw1O4+vHdOX0wV349l8/5bnZGw47KWwrKOT+t5cxvn8nLqlHQjlcSgoiIg1g/fa9PDJzFacNTuPqE/pw0sDOB1QVfXdcOvf+exlLt+xmWI/2YV/33jeXUVhSzn0XD6cxFqNUQ7OISAN4bvYGEuKMP1wygrOGdj2k7eCSsb1olRDHc7NrXN/mELNWbOP1BVv40RkDGJDWrqFDrpaSgojIEdpfXMbUzCzOObYbXdonV3tMapskzh/Rg9fmbWZPUWm1xxx8zV9MW0z/tLb88PQBDR1yjZQURESO0BsLt5C/v4Tvje9T63FXjk9nb3EZr83bXOc1H565ik079/M/Fw+nVUJkxiRUR0lBROQIPfv5BgZ3bce4fp1qPW5071SGdW/Pc7M3UtvCksuyd/P4R2u5LKMX4/sf1dDh1kpJQUTkCCzYlMfCrHy+N75PnQ3BZsaV49NZlr2beZuqn9CuLBiTkNo6kf8+b2gkQq6VkoKIyBH4x+cbaJsUz0Wje4Z1/IWjetKuVQLPfl59g/Nzszcwf1Med58/lNQ2kR2TUB0lBRGRetq1t5g3Fmzh4jE9SUlODOucdq0SuGh0D95cmE3evuID9m3NL+SB6Ss4ZVBnLhoVXpJpaBFLCmaWbGZfmNkCM1tiZvcE5U+b2Tozmx88RgXlZmaPmNlqM1toZmMiFZuISEN4ee4mikrLuaqOBuaDXTmuD8Wl5bwyN+uA8nveWEJJWTn3XnRso4xJqE4k7xSKgDPdfSQwCphgZuODfT9z91HBY35Qdi4wKHhMAv4awdhERI5Iebnz7OcbOb5vJ4Z0C38wGsDQ7u0Z26cjz1dpcP7P0hzeXryVm88aRJ+j2kYi5LBELCl4yJ5gMzF41NzcDhcCzwTnfQ6kmln3SMUnInIkPlyVy8ad+/jeCYd3l1DhynHprN2+l8/W7GBvUSm/nLaYo7umMOnU/g0c6eGJaJuCmcWb2XxgGzDD3WcHu+4LqogeMrNWQVlPYFOV07OCsoOvOcnMMs0sMzc3N5Lhi4jU6NnPN9C5XSvOOaZbvc4/b3h3Utsk8tzsjfzp3ZVk7y7kf741nMT46Db1RvTd3b3M3UcBvYDjzexY4C5gCHAc0An4eXB4dRVoh9xZuPtkd89w94y0tLQIRS4iUrNNO/cxc/k2rji+N0kJ9fszmpwYz6VjezF9yVae/nQdV45LZ2yfjg0c6eFrlJTk7nnALGCCu2cHVURFwFPA8cFhWUDvKqf1ArY0RnwiIofjhS82YsAVx6cf0XW+O64PZeVO53atuGPCkIYJ7ghFsvdRmpmlBq9bA2cDyyvaCSzUtH4RsDg45XXg6qAX0ngg392zIxWfiEh9FJWW8dKcTZw9tCs9Ulsf0bX6dW7LL88fxqNXjKZ9mF1aIy2SU2d3B6aYWTyh5DPV3d80s/fMLI1QddF84Mbg+LeA84DVwD7g2gjGJiJy2MrKnXvfXMaOvcX1bmA+2PdP7tcg12koEUsK7r4QGF1N+Zk1HO/ATZGKR0Ri07yNu/jDOysoLCnjotE9OX9Ej3qtXlZYUsYtL87jnSU5TDq1PydHcJ3kaNIiOyLSIm3J288D05fz2vwtdG7Xis7tkvjltCX89s2lnHF0F741phdnDEkLawbSnXuL+cGUOczblMevLhjGtSc1rV/3DUlJQURalL1FpfztgzVM/mgt5Q43nTGAH54+kHatEli6ZTevzsvitflbeHdpDqltEjl/RHcuGNGDjL6diI87tBPkxh37uOapL8jK289fvjuGc4e37OFTVtv0rU1dRkaGZ2ZmRjsMEWkC3J1/fbmZB95ZTs7uIi4Y2YOfTziaXh3bHHJsaVk5n6zZwb++zOKdJVspLCknLaUV5x7bjfOGd+e4IEEszMrj+0/PobTc+fvVGWT0rX1q7ObCzOa6e0a1+5QURKS5yy0o4mevLGDWilxG9U7lF+cPC7vP/96iUt5bvo23FmXz3vJtFJWGEsTpg9P496JsOrVN4ulrj2dgl8ZZDrMx1JYUVH0kIs3arBXb+OnLCygoLOU3Fx7DVeP6EFdNNVBN2rZK4IKRPbhgZI8DEsTrC7YwpFsKj0/MoEtK9UtstkRKCiLSLBWWlPHA9BU8+ck6hnRL4fnrxzO4a8oRXbNqgigpKychzqI2W2m0KCmISLOzKqeAH78wj+VbC7jmxL7cee4QkhMbdh3jaM9BFC1KCiLSrEybv5k7XllIu1YJPHXNcZwxpEu0Q2pRlBREpNlYvW0PP3tlIaN6pfK/V46Oqbr+xhKb90ci0uyUlTs/fXkBbZLilRAiSHcKItIsPP7RWuZvyuPhy0cpIUSQ7hREpMlbva2AB2es5JxjuvLNkT2iHU6LpqQgIk1aaVk5t7+8kLZJ8dx70fCY6yLa2FR9JCJN2uMfrWPBpjweuWI0aSmt6j5BjkhYSSFYE6Fr1ePdfWOkghIRgdB4hIdmrGTCMd24YETLnoiuqagzKZjZj4FfATlAeVDswIgIxiUiMS5UbbSAtq3i+e1Fx6raqJGEc6dwC3C0u++IdDAiIhX+9uFaFmbl87/fVbVRYwqnoXkTkB/pQEREKqzYWsDD/1nFecO7cf4I9TZqTDXeKZjZbcHLtcAsM/s3UFSx390fjHBsIhKDikvLufWl+aQkJ/CbC4+Ndjgxp7Y7hZTgsRGYASRVKatzKkIzSzazL8xsgZktMbN7gvJ+ZjbbzFaZ2UtmlhSUtwq2Vwf7+x7ZRxOR5ujP/1nJsuzd3P/tEXRup2qjxlbjnYK733OE1y4CznT3PWaWCHxsZm8DtwEPufuLZvYYcB3w1+B5l7sPNLPLgd8D3znCGESkGZm7YSePfbCGyzJ68bVhXaMdTkyqs03BzGaYWWqV7Y5m9k5d53nInmAzMXg4cCbwSlA+BbgoeH1hsE2w/yxTdwORmLG3qJSfvLSAHqmt+cX5w6IdTswKp6E5zd3zKjbcfRcQ1ly1ZhZvZvOBbYSqoNYAee5eGhySBfQMXvck1KhNsD8fOKqaa04ys0wzy8zNzQ0nDBFpBu799zI27drHny4dSUpyYrTDiVnhJIUyM0uv2DCzPoR+8dfJ3cvcfRTQCzgeGFrdYRWXrmVf1WtOdvcMd89IS0sLJwwRaeLeX76NF77YyPWn9Gdc/0N+C0ojCmecwv8j1B7wQbB9KnDD4byJu+eZ2SxgPJBqZgnB3UAvYEtwWBbQG8gyswSgA7DzcN5HRJqfnXuLueOfCzm6awq3fW1wtMOJeXXeKbj7dGAM8BIwFRgblNXKzNIq2iLMrDVwNrAMeB+4JDhsIjAteP16sE2w/z13D+uORESaJ3fn7tcWkbevmIe+M6rBl9SUwxfONBcz3f0s4M1qymrTHZgSzJsUB0x19zfNbCnwopndC8wDngiOfwL4h5mtJnSHcPnhfxwRaU6mzd/CW4u2cseEoxnWo320wxFqH7yWDLQBOptZR76q828P1DnE0N0XAqOrKV9LqH3h4PJC4NLwwhaR5q683HlwxkqG9+zADacOiHY4EqjtTuEG4FZCCWAuXyWF3cD/RTguEWnhPliZy8ad+3j0itHEx6n3eVNR2+C1h4GHzezH7v5oI8YkIjFgymfr6ZLSinOO6RbtUKSKOtsU3P1RMzsWGAYkVyl/JpKBiUjLtX77XmatyOWWswaRlKAFIJuScBqafwWcTigpvAWcC3wMKCmISL384/MNJMQZ3x2XXvfB0qjCSdGXAGcBW939WmAkoFmqRKRe9hWX8nLmJiYc242u7ZPrPkEaVThJYb+7lwOlZtae0JQV/SMbloi0VNPmb2F3YSkTT+wb7VCkGuGMaM4MBqE9TqgX0h7gi4hGJSItkrsz5dP1DO3enow+HaMdjlQjnIbmHwUvHzOz6UD7YAyCiMhhydywi+VbC/jdt4ZrzeUmKpw7BczsW8DJhCao+xhQUhCRA1TMSlPbH/spn66nfXICF47SEptNVTjrKfwFuBFYBCwGbjAzDV4TkQPc//ZyTv79+8xZX/08ljm7C5m+eCuXZfSmTVJYv0clCsL5L3MacGzF5HRmNoVQghARAaC0rJypmZvYta+Eyyd/zh3nHM31p/QnrspI5ednb6TMnavG94lipFKXcHofrQCqdibujaqPRKSKL9bvZNe+Eh749gi+Pqwrv3t7OZP+kUnevmIAikvLef6LjZw2OI2+ndtGOVqpTW0T4r1BqA2hA7DMzL4ItscBnzZOeCLSHLyzeCvJiXGcP7I7l2b0Ysqn67nvrWV845GP+b8rx7Bp5z5yC4qYeELfaIcqdait+uiPjRaFiDRb5eXO9CVbOW1wWmVbwTUn9WNUekdueu5LLn3sU9LatSK9UxtOG6zVEpu62ibE+6CmfSIiFeZn5ZGzu4gJxx44sd2o3qn8++aTuX3qAmYu38bd3xh6QBuDNE3qAiAiR2T64q0kxhtnDul6yL7UNkk8fnUGX27cxeh0DVZrDpQURKTe3J3pi7dy4oDOdGidWO0xcXFGRt9OjRyZ1NdhzVlrZh3NbESkghGR5mVZdgEbd+47pOpImq9wBq/NMrP2ZtYJWAA8ZWYPRj40EWnqpi/OJs7ga8MOrTqS5imcO4UO7r4b+BbwlLuPBc6u6yQz621m75vZMjNbYma3BOW/NrPNZjY/eJxX5Zy7zGy1ma0ws3Pq+6FEpHFMX7KV4/p2onM7zabfUoTTppBgZt2By4D/dxjXLgVud/cvzSwFmGtmM4J9D7n7AV1ezWwYcDlwDKF1of9jZoPdveww3lNEGsma3D2szNnDry4YFu1QpAGFc6fwG+AdYI27zzGz/sCquk5y92x3/zJ4XQAsA3rWcsqFwIvuXuTu64DVwPFhxCciUTB98VYArbHcwtSZFNz9ZXcf4e4/DLbXuvu3D+dNzKwvMBqYHRT9l5ktNLMnzayin1pPYFOV07KoJomY2SQzyzSzzNzc3MMJQ0Qa0DtLtjKydyo9UltHOxRpQOE0NA82s5lmtjjYHmFmd4f7BmbWDvgncGvQNvFXYAAwCsgG/lRxaDWn+yEF7pPdPcPdM9LSNDpSJBqydu1jYVY+56rXUYsTTvXR48BdQAlAsMDO5eFc3MwSCSWE59z9X8H5Oe5eFizx+ThfVRFlEZpsr0IvYEs47yMijeudJTmAqo5aonCSQht3P3j5zdK6TrLQShtPAMvc/cEq5d2rHHYxoTUaAF4HLjezVmbWDxiElv0UaZLeWbyVId1S6KcZT1uccHofbTezAQRVOWZ2CaFqn7qcBHwPWGRm84Oy/wauMLNRwfXWAzcAuPsSM5sKLCWUdG5SzyORpmdbQSFzNuzklrMGRTsUiYBwksJNwGRgiJltBtYBV9V1krt/TPXtBG/Vcs59wH1hxCQiUTJjaQ7uaBRzC1VnUnD3tcDZZtYWiAu6l4pIjJq+eCv9Orfl6K4p0Q5FIqC2RXZuq6EcgKrtBCISG9bm7uGzNTv4wSn9K/8WSMtS251Cxc+Ao4HjCDUEA1wAfBjJoESk6dm1t5jvPz2HDq0T+d4JWme5paptkZ17AMzsXWBMRbWRmf0aeLlRohORJqG4tJwbn53LlrxCXpg0jp4asNZihdMlNR0orrJdDPSNSDQi0mj2FJVyxeTPeXTmKsrKDxknWsndufu1Rcxet5MHLhnB2D5aG6ElC6f30T+AL8zs1WD7ImBK5EISkcbwxEfr+GztDj5bu4MPV+Xy4GWj6N2pzSHH/e3DtUzNzOLmMwdy0ejapi+TliCcuY/uA64FdgE7gWvd/XeRDkxEImfX3mIe/2gt5xzTlYe+M5Jl2QWc9/BHTJu/+YDjpi/eyu+nL+f8Ed35ydcGRylaaUzhrrxWBpRXeYhIM/bYB2vYW1zKT79+NBeP7sXbt5zC4G4p3PLifG59cR67C0tYvDmfn7w0n5G9UvnjpSPV2yhG1Fl9FCyOcz2hOYwMeNbMJrv7o5EOTkQa3tb8Qp7+dD0Xj+7JoGCsQe9ObXhp0nj+MmsND89cxZz1uygtL6dT2yQmXz2W5MT4KEctjSWcO4XrgHHu/it3/yUwnlCSEJFm6NH3VlHuzk/OPrA6KCE+jpvPGsTLN55AfJyxp7CUv0/MoEtKcpQilWgIp6HZCFUfVSij+ukrRKSJ27BjLy/N2cR3x6VX26gMMCa9I+/+5FQKCktJS9Eym7EmnKTwFDA76H1khFZIeyKiUYlIRDw0YyUJ8cZ/nTGw1uOSE+NVZRSjwpn76EEzmwWcTCgpXOvu8yIdmIg0rOVbdzNtwRZuOHUAXdqrSkiqF05D8wBgibt/aWanA6eY2Tp3z4t4dCLSYP707kratUrgxtP6RzsUacLCaWj+J1BmZgOBvwP9gOcjGpWINKgvN+5ixtIcbji1P6ltkqIdjjRh4SSFcncvBb4FPOzuPwG613GOiDQhf3xnBZ3bJXHtSf2iHYo0ceEkhRIzuwK4GngzKEuMXEgi0pA+Wb2dT9fs4KYzBtK2VTh9SySWhZMUrgVOAO5z93XB+snPRjYsEWkoj32whm7tk/nuuPRohyLNQDhzHy1195vd/YVge52731/XeWbW28zeN7NlZrYkGBmNmXUysxlmtip47hiUm5k9YmarzWyhmY050g8nEutWb9vDR6u2c9X4dFolqIup1K3GpGBmU4PnRcEf6YrHIjNbGMa1S4Hb3X0ooVHQN5nZMOBOYKa7DwJmBtsA5wKDgsck4K/1/lQiAsAzn60nKT6Oy4/XXYKEp7YKxluC5/Prc2F3zwayg9cFZrYM6Elo8NvpwWFTgFnAz4PyZ9zdgc/NLNXMugfXEZHDVFBYwj/nZnH+yO50bqeRyRKeGu8UKv4Yu/sGoAgYCYwAioKysJlZX2A0MBvoWuXa2UCX4LCewKYqp2UFZQdfa5KZZZpZZm5u7uGEIRJTXpmbxd7iMq45sW+0Q5FmpM42BTP7AfAFoS6plxD6Ff/9cN/AzNoRGutwq7vvru3QasoOWQ7K3Se7e4a7Z6SlpYUbhkhMKS93nvlsA6PTUxnRKzXa4UgzEk7/tJ8Bo919B4CZHQV8CjxZ14lmlkgoITzn7v8KinMqqoXMrDuwLSjPAnpXOb0XsCW8jyEiVX20ejvrtu/l4ctHRTsUaWbC6ZKaBRRU2S7gwGqealloRY4ngGXu/mCVXa8DE4PXE4FpVcqvDnohjQfy1Z4gUj9TPl1P53atOPdYjTOVwxPOncJmQrOkTiNUnXMhoTWbb4PQhHk1nHcS8D1gkZnND8r+G7gfmGpm1wEbgUuDfW8B5wGrgX2ExkeIyGHasGMv76/Yxo/PHERSQriLK4qEhJMU1gSPChW/7FNqO8ndP6bmdRfOquZ4B24KIx4RqcUzn20g3owrNVhN6iGcqbPvATCztu6+N/IhiUh97S0qZWrmJs4d3p2umh5b6iGc3kcnmNlSYFmwPdLM/hLxyETksL06bzMFhaVcc2KfaIcizVQ4FY5/Bs4BdgC4+wLg1EgGJSKHKi0r58mP1/HDZ+fy6rws9heXHbDf3Xnms/Uc27M9Y9I7RidIafbCmjLR3TeFOhNVKqvpWBFpeJnrd3L3a4tZvrWAjm0SeXvxVn7Zagnnj+zOJWN7MyY9lc/W7GBlzh7+cMkIDvr3KhK2cJLCJjM7EXAzSwJuJqhKEpHI2rm3mPvfXsbUzCy6d0jmsavG8PVh3fhi/U6mZm7itXlbeOGLTQxIa0tifBwd2yRywcge0Q5bmrFwksKNwMOEppzIAt5FvYREIqq83HlxziYeeGc5ewpLueG0/tx85qDK9RDG9z+K8f2P4jcXlvLvhVt4OTOLzA27uPnMgSQnajZUqT8L9QRtnjIyMjwzMzPaYYg0uGue+oJZK3IZ168Tv73oWAZ3rbUHOADbCgrp1CaJhHiNTZDamdlcd8+obp+WYRJpYrYVFDJrRS6TTu3PXecOCbt9oEuKuqDKkdNPCpEmZvHmfADOHtpVDcbS6GpbZKdipbSTGi8cEVmUtRszOKZH+2iHIjGotjuFirmHHm2MQEQkZNHmfPp3blvZqCzSmGr7v26Zma0H0g5aftMITVU0IqKRicSoxZvzGd+/U7TDkBhVY1Jw9yvMrBvwDvDNxgtJJHblFhSxdXchx/bsEO1QJEbVen/q7luBkcGgtcFB8Qp3L4l4ZCIxqKKRebiSgkRJnZWWZnYa8AywnlDVUW8zm+juH0Y4NpGYs2hzfqiRWUlBoiSclqwHga+7+woAMxsMvACMjWRgIrFo0eZ8+nVuSzs1MkuUhDNOIbEiIQC4+0ogMXIhicSuxZvzVXUkURXOz5FMM3sC+EewfSUwN3IhicSm7XuKyM4vVFKQqAonKfyQ0AR4NxNqU/gQ0CI7Ig1sUdDIrJ5HEk11Vh+5e5G7P+ju33L3i939IXcvqus8M3vSzLaZ2eIqZb82s81mNj94nFdl311mttrMVpjZOfX/SCLN0+KsUFLQSGaJpkjOffQ0MKGa8ofcfVTweAvAzIYBlwPHBOf8xcw0/6/ElIqRzCnJarKT6IlYUgi6rO4M8/ALgReDu5J1wGrg+EjFJtIULd6cr6ojibpozJL6X2a2MKheqlhItiewqcoxWUHZIcxskpllmllmbm5upGMVaRQ79hSxRY3M0gTUmRTMbLCZPW5m75rZexWPer7fX4EBwCggG/hTxdtUc2y1q/+4+2R3z3D3jLS0tHqGIdK0qJFZmopweh+9DDwGPA6UHcmbuXtOxWszexx4M9jMAnpXObQXsOVI3kukOamY3uKYnmpklugKJymUuvtfG+LNzKy7u2cHmxcDFT2TXgeeN7MHgR7AIOCLhnhPkUjZV1zKqpw9rMgpYOXWArbk76dNUgIpyQmkJCfSPjmB9smJtG+dwGmDu9A6qea+ExUjmdurkVmirMakYGYVc/e+YWY/Al4FKruiunutjchm9gJwOtDZzLKAXwGnm9koQlVD64EbgmstMbOpwFKgFLjJ3Y/orkQkEl5fsIXX529mRU4Bm3buryxvlRBHz9TW7C8po6CwlD1FpQecd/bQrvx9YrVL4gKwePNuxvTpWON+kcZS253CXEJ/vCvq+39WZZ8D/Wu7sLtfUU3xE7Ucfx9wX23XFImm7Pz93PbSfLq2T2Z0eiqXje3N4G4pDO6aQnqnNsTHfdU0Vlbu7CksZXdhCc/N3shjH6xhUVY+w3sd2mawc28xm/P2M/HEPo35cUSqVdt6Cv0AzCzZ3Qur7jMzrRAuMefpT9bjwIuTxvGGSYoAABKySURBVNO7U5taj42PMzq0SaRDm0RuOmMAL3yxkYdnruTvE4875Fg1MktTEk6X1E/DLBNpsQoKS3h+9kbOG969zoRwsJTkRK4/pR//WbaNRcGo5aoWKylIE1JjUjCzbmY2FmhtZqPNbEzwOB04vH8VIs3cS3M2UVBUyvWn9KvX+RNP7EuH1ok8PHPlIfsWZeXT96g2amSWJqG2NoVzgGsIdQ99sEp5AfDfEYxJpEkpKSvnyY/XMa5fJ0b0Sq3XNSruFv747spD2hYWbc5ndHr9rivS0Gq8U3D3Ke5+BnCNu59R5fFNd/9XI8YoElVvLcpmS34hk06ttW9Fnaq7W9gVNDJrJLM0FeGMU+hjZrcdVJYPzHX3+RGISaTJcHcmf7iWAWltOePoLkd0reruFhZpTWZpYsJpaM4AbiQ0F1FPYBKh8QePm9kdkQtNJPo+W7ODJVt2c/0p/YmLq242lsNz8N3CosqRzEoK0jSEkxSOAsa4++3ufjuhJJEGnEqozUGkxZr80Vo6t0viotHVzs942A7uibR4cz59jmpDh9ZqZJamIZykkA4UV9kuAfq4+36qjHAWaWlWbC1g1opcJp7Ql+TEhlveo+rdwiJNly1NTDhtCs8Dn5vZtGD7AuAFM2tLaFoKkRbp7x+tJTkxjqvGN+xI46ptC0CDX1/kSISzHOdvCbUj5BFqYL7R3X/j7nvd/cpIBygSDdt2F/La/M1cltGbjm2TGvz6FXcLoEZmaVrCXWRnHqEptP8FbDOz9MiFJBJ9T3+6ntJy57qT6zdYrS4pyYn88PQBtEmKV/WRNCl1Vh+Z2Y8JzXCaQ2g9BSM0Id6IyIYmEh17i0p59vMNTDimG32Oahux97nh1P5cflxvNTJLkxJOm8ItwNHuviPSwYg0BY/MXMXuwlKuP8LBanUxM1LbNHzVlMiRCKf6aBOhtgSRFu+jVbn87cO1XHF8OmPStb6BxJ5w7hTWArPM7N8cuMjOgzWfItL87NhTxG1TFzCwSzt+ef6waIcjEhXhJIWNwSMpeIi0OO7Oz15ZSP7+Ep75/vG1Lp0p0pLVmRTc/R4AM2vr7nsjH5JI45vy6XreW76NX18wjKHd20c7HJGoqbNNwcxOMLOlwLJge6SZ/SXikYk0kmXZu/mft5dz1pAuTDyxb7TDEYmqcBqa/0xobYUdAO6+gNC8R7UysyfNbJuZLa5S1snMZpjZquC5Y1BuZvaIma02s4VmNqZ+H0fk8OwvLuPHL8wjtXUiD1wyArMjn/ROpDkLa/Cau286qKgsjNOeBiYcVHYnMNPdBwEzg22Ac4FBwWMS8Ndw4hI5Ur/991LW5O7hwctGcVS7VtEORyTqwuqSamYnAm5mSWb2U4KqpNq4+4fAzoOKLwSmBK+nABdVKX/GQz4HUs2se1ifQKSepi/eyvOzNzLp1P6cPKhztMMRaRLCSQo3AjcRWkshCxgF/Kie79fV3bMBgueKVUt6EhoPUSErKDuEmU0ys0wzy8zNza1nGBLr9haVcvdrixneswO3f+3oaIcj0mSEMyHedne/0t27unsXd78KuLqB46iuItdriGeyu2e4e0ZaWloDhyGx4omP17F9TxH3XHgMSQnhTgEm0vLV91/DwctzhiunolooeN4WlGcBvasc1wvYUs/3EKnVjj1FTP5wLecc01WjlkUOUt+kUN8uGq8DE4PXE4FpVcqvDnohjQfyK6qZRBra/76/mn3FpfzsnCHRDkWkyQlnRHN1qq3aqcrMXiC0lnNnM8siNNPq/cBUM7uO0CjpS4PD3wLOA1YD+4Br6xmXSK027dzHc59v5LKM3gzs0i7a4Yg0OTUmBTMroPo//ga0ruvC7n5FDbvOquZYJ9SYLRJRD81YiRncevbgaIci0iTVmBTcPaUxAxGJtGXZu3l1/mYmndqfbh2Sox2OSJOkbhcSMx6YvpyUVgn86LSB0Q5FpMlSUpCY8PnaHby/IpcfnTGQDm200plITZQUpMVzd+5/eznd2idzjSa8E6mVkoK0eO8syWH+pjxuPXsQyYlaJ0GkNkoK0qLt2FPEH95ZzoC0tlwytle0wxFp8uo7TkGkSdlfXMaKnAJWbN3Niq17WJETet6+J7SC7N++N5aEeP0GEqmLkoI0O4UlZSzN3s2irHwWZuWzeHM+q7YVUB6MqmmdGM/gru044+g0ju6Wwuj0VMb26RTdoEWaCSUFaTbeX76NP81YwbLsAsqCDNC5XRIjeqVyzrHdOKZHe4Z0S6F3xzbExWmxHJH6UFKQJi9/fwm/fXMpr8zNYkBaW350+gCO7dmBEb060K19slZLE2lASgrSpL2/Yht3/XMR2woK+dHpA7jl7EG0SlAPIpFIUVKQJil/fwn3vrmUl+dmMahLO/72vZMY2Ts12mGJtHhKCtKklJaVM33JVu59c5nuDkSiQElBmoT8fSW8OGcjz3y2gc15+xncVXcHItGgpCARVVpWzs69xbRvnVjtaOLV2wp46pP1/OvLzewvKWN8/0788oJhnD20K/HqQSTS6JQUJGIWb87n5hfnsTZ3LwDJiXGktk4itU0iHVonUlbuZG7YRVJCHBeO7MG1J/VjWI/2UY5aJLYpKcgBZizN4cEZK7n2xL5cdlzvuk+oRnm58+Qn6/j99OUc1bYVd39jKEWl5eTvLyFvXzF5+0rI219CcVk5t39tMN8dl85R7Vo18CcRkfpQUhAAdu0t5tdvLGHa/C20TYrnjn8upLC0jKtP6HtY18ktKOL2lxfw4cpcvj6sK7//9gg6tk2KTNAi0uCikhTMbD1QAJQBpe6eYWadgJeAvsB64DJ33xWN+GLNW4uy+eW0xeTtK+HWswdx/Sn9ufWl+fxy2hKKSsq5/tT+YV1n1opt/PTlBRQUlnLvRcdy5bh0DSwTaWaieadwhrtvr7J9JzDT3e83szuD7Z9HJ7TYkFtQxK9eX8xbi7YyvGcH/nHdOIZ2D9Xp/+XKMdz64nzue2sZRaVl/NeZg2q8zo49RTz63mqe/nQ9Q7ql8Pz14xncVau5ijRHTan66ELg9OD1FGAWSgoRUV7uvPJlFr97axl7i8q4Y8LRTDql/wGziCbGx/Hw5aNolRDHH99dSVFpObd9bfABv/yXZe/mqU/W8dr8LRSXljPxhD7cdd5QrVkg0oxFKyk48K6ZOfA3d58MdHX3bAB3zzazLtWdaGaTgEkA6enpjRVvizFn/U5+88ZSFm3OZ0x6Kg9cMoKBXar/VZ8QH8cfLh1JUkIcj763msKSMu48dygzl+Xw5Cfr+HztTlonxnPp2F5cc2JfBunuQKTZi1ZSOMndtwR/+GeY2fJwTwwSyGSAjIwMj1SALU3Wrn387u3l/HthNt07JPPw5aP45sgeddb5x8cZ/3PxcJIS4nj8o3W8MjeLXftK6NEhmTvPHcLlx/UmtY0akkVaiqgkBXffEjxvM7NXgeOBHDPrHtwldAe2RSO2lmZvUSmPfbCGyR+uxQxuOWsQN5zWnzZJ4f+nj4sz7vnmMaS2TmTuxl1cOa4PXx/WVYvWiLRAjZ4UzKwtEOfuBcHrrwO/AV4HJgL3B8/TGju2lqK83Jm3aRdvLMjmjQVb2LG3mAtH9eDnE4bQI7V1va5pZtz29aMbOFIRaWqicafQFXg1qLZIAJ539+lmNgeYambXARuBS6MQW5NTWFJG5vpdfLpmO3PW76RdqwQGd0vh6K4pDO6awsAu7UhOjMfdWbJlN28s2MKbC7PZnLefVglxnHF0F64/tT9j+3SM9kcRkWag0ZOCu68FRlZTvgM4q7HjaWpCv/Lz+HT1dj5Zs50vN+RRXFZOQpwxvFcHsvML+WT1DorLygGIM+hzVFvK3dmwYx8Jccapg9P46TmDOXtoV1KSE6P8iUSkOWlKXVJj3q69xdz84jw+WrUdMxjWvT0TT+zDiQM7c1zfTrRrFfrPVVJWzoYde1mZs4cVWwtYmVNAYUkZPzxtABOO7aaGXxGpNyWFJmLplt3c8GwmOflF/OqCYVw0qmeN00MkxscxsEsKA7ukcN7w7o0cqYi0ZEoKTcDrC7ZwxysL6NA6kZduGM/odNX/i0h0KCkchqLSsgZdAay0rJwH3lnB5A/XktGnI3+5agxdUpIb7PoiIodLSaEG+4pLWZSVz/xNeczflMe8jXls3V1I26R4urRPpktKq6+eU1qRnBhPWblT7qFHWTmUuxNnRsc2iXRsm0Sntkl0bBN6Lnfn1hfn8/Hq7XxvfB9+cf4wkhLU719EoktJIbCvuJTP1uzgg5W5ZK7fxYqcAsrKQwOm0zu14fh+nRjYpR15+0rIKSgkd3cRi7LyyNldxP6Ssnq9Z1J8HA98e0S91y0QEWloMZsU3J2VOXv4YOU2PliZy5x1uyguK6dNUjxj+3TkpqEDGJWeysheqbUuAOPu7CkqpaTMiTfD4iDejDgz4uKgvBx27Stm597iyuede4vJ31/C2UO7cmzPDo34qUVEaheTSeG95Tn8v1cXk51fCMCQbilce1JfThucxti+HQ+r3cDM6hwL0Dqpdb1HEouINKaYTApd2yczOj2VWwencergNLp30B9sERGI0aRwTI8O/OXKsdEOQ0SkyVF3FxERqaSkICIilZQURESkkpKCiIhUUlIQEZFKSgoiIlJJSUFERCopKYiISCVz92jHUG9mlgtsqOfpnYHtDRhOc6TvQN8B6DuIxc/fx93TqtvRrJPCkTCzTHfPiHYc0aTvQN8B6DuI9c9/MFUfiYhIJSUFERGpFMtJYXK0A2gC9B3oOwB9B7H++Q8Qs20KIiJyqFi+UxARkYMoKYiISKWYTApmNsHMVpjZajO7M9rxNAYze9LMtpnZ4iplncxshpmtCp47RjPGSDKz3mb2vpktM7MlZnZLUB5L30GymX1hZguC7+CeoLyfmc0OvoOXzCwp2rFGmpnFm9k8M3sz2I6576AmMZcUzCwe+D/gXGAYcIWZDYtuVI3iaWDCQWV3AjPdfRAwM9huqUqB2919KDAeuCn47x5L30ERcKa7jwRGARPMbDzwe+Ch4DvYBVwXxRgbyy3AsirbsfgdVCvmkgJwPLDa3de6ezHwInBhlGOKOHf/ENh5UPGFwJTg9RTgokYNqhG5e7a7fxm8LiD0B6EnsfUduLvvCTYTg4cDZwKvBOUt+jsAMLNewDeAvwfbRox9B7WJxaTQE9hUZTsrKItFXd09G0J/NIEuUY6nUZhZX2A0MJsY+w6CapP5wDZgBrAGyHP30uCQWPj38GfgDqA82D6K2PsOahSLScGqKVO/3BhhZu2AfwK3uvvuaMfT2Ny9zN1HAb0I3TUPre6wxo2q8ZjZ+cA2d59btbiaQ1vsd1CXhGgHEAVZQO8q272ALVGKJdpyzKy7u2ebWXdCvx5bLDNLJJQQnnP3fwXFMfUdVHD3PDObRah9JdXMEoJfyi3938NJwDfN7DwgGWhP6M4hlr6DWsXincIcYFDQ2yAJuBx4PcoxRcvrwMTg9URgWhRjiaig3vgJYJm7P1hlVyx9B2lmlhq8bg2cTaht5X3gkuCwFv0duPtd7t7L3fsS+rf/nrtfSQx9B3WJyRHNwa+EPwPxwJPufl+UQ4o4M3sBOJ3QNME5wK+A14CpQDqwEbjU3Q9ujG4RzOxk4CNgEV/VJf83oXaFWPkORhBqRI0n9INwqrv/xsz6E+pw0QmYB1zl7kXRi7RxmNnpwE/d/fxY/Q6qE5NJQUREqheL1UciIlIDJQUREamkpCAiIpWUFEREpJKSgoiIVFJSEKmGmZWZ2fwqjwabKM/M+ladrVakKYnFEc0i4dgfTAchElN0pyByGMxsvZn9PliX4AszGxiU9zGzmWa2MHhOD8q7mtmrwRoGC8zsxOBS8Wb2eLCuwbvBCGPM7GYzWxpc58UofUyJYUoKItVrfVD10Xeq7Nvt7scD/0toZDzB62fcfQTwHPBIUP4I8EGwhsEYYElQPgj4P3c/BsgDvh2U3wmMDq5zY6Q+nEhNNKJZpBpmtsfd21VTvp7QQjVrgwn2trr7UWa2Heju7iVBeba7dzazXKBX1SkTgqm7ZwQLumBmPwcS3f1eM5sO7CE0BclrVdY/EGkUulMQOXxew+uajqlO1Xl1yviqfe8bhFYGHAvMNTO1+0mjUlIQOXzfqfL8WfD6U0KzbgJcCXwcvJ4J/BAqF7hpX9NFzSwO6O3u7xNaBCYVOORuRSSS9CtEpHqtgxXKKkx394puqa3MbDahH1VXBGU3A0+a2c+AXODaoPwWYLKZXUfojuCHQHYN7xkPPGtmHQgt/PKQu+c12CcSCYPaFEQOQ9CmkOHu26Mdi0gkqPpIREQq6U5BREQq6U5BREQqKSmIiEglJQUREamkpCAiIpWUFEREpNL/B6RkGHcK/t4zAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Final plot\n",
    "plt.figure(2)\n",
    "plt.title('Training...')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Length of episodes batch')\n",
    "plt.plot(torch.Tensor(all_epoch_lengths).numpy())\n",
    "print(\"Last episode length is {}, epoch is {}\".format(epoch_episode_length, epoch))\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run it longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1306\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env._max_episode_steps = 5000\n",
    "\n",
    "state = env.reset() # reset environment\n",
    "for t in range(env._max_episode_steps): # rollout one episode until it finishes or stop after 200 steps\n",
    "    state_pytorch = torch.from_numpy(state).float().unsqueeze(0) # state=s\n",
    "    action, _ = policy_net.eval().select_action(state_pytorch)\n",
    "    state, reward, done, _ = env.step(action) # receive next state=s' and reward=r\n",
    "    env.render() # visualize state\n",
    "    if done:\n",
    "        print(t)\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
